{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 7s - loss: 0.6860 - binary_crossentropy: 0.6860 - val_loss: 0.6547 - val_binary_crossentropy: 0.6547\n",
      "Epoch 2/10\n",
      "1/1 - 0s - loss: 0.6664 - binary_crossentropy: 0.6664 - val_loss: 0.6441 - val_binary_crossentropy: 0.6441\n",
      "Epoch 3/10\n",
      "1/1 - 0s - loss: 0.6473 - binary_crossentropy: 0.6473 - val_loss: 0.6335 - val_binary_crossentropy: 0.6335\n",
      "Epoch 4/10\n",
      "1/1 - 0s - loss: 0.6283 - binary_crossentropy: 0.6282 - val_loss: 0.6230 - val_binary_crossentropy: 0.6230\n",
      "Epoch 5/10\n",
      "1/1 - 0s - loss: 0.6091 - binary_crossentropy: 0.6091 - val_loss: 0.6130 - val_binary_crossentropy: 0.6130\n",
      "Epoch 6/10\n",
      "1/1 - 0s - loss: 0.5901 - binary_crossentropy: 0.5901 - val_loss: 0.6037 - val_binary_crossentropy: 0.6037\n",
      "Epoch 7/10\n",
      "1/1 - 0s - loss: 0.5713 - binary_crossentropy: 0.5712 - val_loss: 0.5953 - val_binary_crossentropy: 0.5953\n",
      "Epoch 8/10\n",
      "1/1 - 0s - loss: 0.5526 - binary_crossentropy: 0.5526 - val_loss: 0.5881 - val_binary_crossentropy: 0.5881\n",
      "Epoch 9/10\n",
      "1/1 - 0s - loss: 0.5342 - binary_crossentropy: 0.5342 - val_loss: 0.5822 - val_binary_crossentropy: 0.5822\n",
      "Epoch 10/10\n",
      "1/1 - 0s - loss: 0.5162 - binary_crossentropy: 0.5161 - val_loss: 0.5780 - val_binary_crossentropy: 0.5780\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6308fdc0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "test LogLoss 0.5577\n",
      "test AUC 0.4839\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x7f6310400c40> it exist\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'C26'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'C26'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fc21c863cd79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msparse_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mlbe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mmms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'C26'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "from deepctr.models import DeepFM\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv('./criteo_sample.txt')\n",
    "\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "    data[dense_features] = data[dense_features].fillna(0, )\n",
    "    target = ['label']\n",
    "\n",
    "    # 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "    mms = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "    # 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "    fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].max() + 1, embedding_dim=4)\n",
    "                              for i, feat in enumerate(sparse_features)] + [DenseFeat(feat, 1, )\n",
    "                                                                            for feat in dense_features]\n",
    "\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    feature_names = get_feature_names(\n",
    "        linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    # 3.generate input data for model\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=2020)\n",
    "    train_model_input = {name: train[name] for name in feature_names}\n",
    "    test_model_input = {name: test[name] for name in feature_names}\n",
    "    # print(\"input test data\", test_model_input)\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary')\n",
    "    model.compile(\"adam\", \"binary_crossentropy\",\n",
    "                  metrics=['binary_crossentropy'], )\n",
    "\n",
    "    history = model.fit(train_model_input, train[target].values,\n",
    "                        batch_size=256, epochs=10, verbose=2, validation_split=0.2, )\n",
    "    pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "    print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "    print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))\n",
    "\n",
    "\n",
    "print(model,\"it exist\")\n",
    "# Define the new data (replace with your new data)\n",
    "new_data = pd.DataFrame({\n",
    "    'I1': [3.0],\n",
    "    'I2': [260.0],\n",
    "    'I3': [0.0],\n",
    "    'I4': [12.0],\n",
    "    'I5': [2013.0],\n",
    "    'I6': [164.0],\n",
    "    'I7': [6.0],\n",
    "    'I8': [35.0],\n",
    "    'I9': [523.0],\n",
    "    'I10': [0.0],\n",
    "    'I11': [3.0],\n",
    "    'I12': [0.0],\n",
    "    'I13': [18.0],\n",
    "    'C1': ['05db9164'],\n",
    "    'C2': ['38a947a1'],\n",
    "    'C3': ['3f55fb72'],\n",
    "    'C4': ['5de245c7'],\n",
    "    'C5': ['30903e74'],\n",
    "    'C6': ['7e0ccccf'],\n",
    "    'C7': ['b72ec13d'],\n",
    "    'C8': ['1f89b562'],\n",
    "    'C9': ['a73ee510'],\n",
    "    'C10': ['acce978c'],\n",
    "    'C11': ['3547565f'],\n",
    "    'C12': ['a5b0521a'],\n",
    "    'C13': ['12880350'],\n",
    "    'C14': ['b28479f6'],\n",
    "    'C15': ['c12fc269'],\n",
    "    'C16': ['95a8919c'],\n",
    "    'C17': ['e5ba7672'],\n",
    "    'C18': ['675c9258'],\n",
    "    'C19': ['21ddcdc9'],\n",
    "    'C20': ['b1252a9d'],\n",
    "    'C21': ['0e8585d2'],\n",
    "    'C22': ['32c7478e'],\n",
    "    'C23': ['0d4a6d1a'],\n",
    "    'C24': ['001f3601'],\n",
    "    'C25': ['92c878de']\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the new data\n",
    "sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "\n",
    "# new_data[sparse_features] = new_data[sparse_features]\n",
    "# new_data[dense_features] = new_data[dense_features]\n",
    "\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    new_data[feat] = lbe.fit_transform(new_data[feat])\n",
    "\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "new_data[dense_features] = mms.fit_transform(new_data[dense_features])\n",
    "\n",
    "# Define feature columns for the model\n",
    "feature_columns = [SparseFeat(feat, vocabulary_size=new_data[feat].max() + 1, embedding_dim=4)\n",
    "                   for feat in sparse_features] + [DenseFeat(feat, 1) for feat in dense_features]\n",
    "\n",
    "feature_names = get_feature_names(feature_columns)\n",
    "\n",
    "# Prepare the input data for model prediction\n",
    "new_data_input = {name: new_data[name] for name in feature_names}\n",
    "\n",
    "# Make predictions for the new data\n",
    "predictions = model.predict(new_data_input)\n",
    "\n",
    "# Interpret the output (predictions) for CTR estimation\n",
    "# The 'predictions' variable now contains the predicted CTR values for the new data points.\n",
    "print(\"Predicted CTR:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
